# Fake-News-Detection
This is a project that detects whether the news is fake or not by examining the title and author of the news. The project was done on Google Colab and is fully in jupyter notebook. Our dataset is outdated so the recent news may give trivial results. And not to mention this is a side project and work is going on it.
The recent project in Data Science helped me to learn more about the intricacies of real world models and project. The project that my group did was in Fake News Detection. We picked up a dataset from kaggle.com and did various kinds of modelling on it. At first we classified the data using simple classifiers like KNeighborsClassifer, SVMs,  Logistic Regression. Then on the second phase we used some ensemble methods on them. We classified them using Random Forests, Adaboost and Xgboost classifiers. The results were all satisfactory but not up to the mark. The best accuracy that we could get was 81% with Random Forest Classifier. 
![Annotation 2021-07-03 123937](https://user-images.githubusercontent.com/61945939/124346343-ddd61080-dbfb-11eb-954a-a3954cb840ab.png)
Then we applied a model of Neural Networks upon the dataset. To our surprise we got an accuracy of 99%. We had to tokenize the data with the help of keras tokenizer. Then we fit in on texts and converted it to sequences. The final dataset was a n-dimensional NumPy array. The array has 45 columns which was the maximum length that was pre-set. After that we trained the model using the training dataset. The accuracy was very good as we have mentioned above. Our model embeds the NumPy array by setting the maximum word size of 40000. The vector features were set at 40 and the input length of the array which was pre-set at 45. Then we used a dropout of 0.3 to avoid any and all chances of overfitting. The model was also given parameters to reduce its learning rate and stop the iterations when the loss did not change much. The optimizer used was Adam with an initial learning rate of 0.001 and the minimum change in loss was set at 0.1. So after a only 6 iterations our model got the best possible parameters and the loss was at its minimum. As we can see the minimum loss and validation loss. Both of them are in the range of 1%. So we conclude by saying that our model gave a very good accuracy but it also had some drawbacks. 
As you can see that the dataset is outdated. So if we give recent news to the model it will give us wrong results. To avoid that we have to take in all the news from almost everywhere. We have to check the leading news websites like  The Hindu, Times of India, Indian Express and also some social media platform like Twitter and Google News etc. And that is the single most important drawback of our project. Adding to that we could also see that our model takes into consideration only one word at a time. So in the future we will try to implement a bigram model. As of now our objectives on the near future is to make a gui that can help us to distribute our model on a larger scale. On the other hand we are also planning to deploy our model on a google server.
If anyone wants to collaborate on this project to make it a better you can reach my mail : binayak5766@gmail.com
